{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip -q install transformers datasets accelerate nvidia-ml-py3 bitsandbytes\n",
        "! nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Llnfo8hY4-p8",
        "outputId": "a96d7011-1d1f-404c-aa30-929912de150c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Apr 30 20:44:13 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   49C    P8    12W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from datasets import Dataset\n",
        "from pynvml import *\n",
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import TrainingArguments, Trainer, logging\n",
        "\n",
        "\n",
        "seq_len, dataset_size = 512, 512\n",
        "dummy_data = {\n",
        "    \"input_ids\": np.random.randint(100, 30000, (dataset_size, seq_len)),\n",
        "    \"labels\": np.random.randint(0, 1, (dataset_size)),\n",
        "}\n",
        "ds = Dataset.from_dict(dummy_data)\n",
        "ds.set_format(\"pt\")\n",
        "\n",
        "def print_gpu_utilization():\n",
        "    nvmlInit()\n",
        "    handle = nvmlDeviceGetHandleByIndex(0)\n",
        "    info = nvmlDeviceGetMemoryInfo(handle)\n",
        "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
        "\n",
        "def print_summary(result):\n",
        "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
        "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
        "    print_gpu_utilization()\n",
        "\n",
        "\n",
        "default_args = {\n",
        "    \"output_dir\": \"tmp\",\n",
        "    \"evaluation_strategy\": \"steps\",\n",
        "    \"num_train_epochs\": 1,\n",
        "    \"log_level\": \"error\",\n",
        "    \"report_to\": \"none\",\n",
        "}\n",
        "\n",
        "print('init:: ', end=' '); print_gpu_utilization()\n",
        "torch.ones((1, 1)).to(\"cuda\")\n",
        "print('cuda init:: ', end=' '); print_gpu_utilization()\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-large-uncased\").to(\"cuda\")\n",
        "print('model on gpu:: ', end=' '); print_gpu_utilization()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DG7zp_TA4fD6",
        "outputId": "09af7142-bb7a-4735-a8c6-f3a47363a48b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "init::  GPU memory occupied: 258 MB.\n",
            "cuda init::  GPU memory occupied: 363 MB.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model on gpu::  GPU memory occupied: 1651 MB.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# vanilla training\n",
        "\n",
        "\n",
        "logging.set_verbosity_error()\n",
        "\n",
        "training_args = TrainingArguments(per_device_train_batch_size=4, **default_args)\n",
        "trainer = Trainer(model=model, args=training_args, train_dataset=ds)\n",
        "result = trainer.train()\n",
        "print_summary(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3mGF4Ea4fHS",
        "outputId": "712d4e6d-6e02-4950-86f6-13691b22e7cd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train_runtime': 176.4357, 'train_samples_per_second': 2.902, 'train_steps_per_second': 0.725, 'train_loss': 0.02635337971150875, 'epoch': 1.0}\n",
            "Time: 176.44\n",
            "Samples/second: 2.90\n",
            "GPU memory occupied: 11537 MB.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# gradient accumulation\n",
        "\n",
        "training_args = TrainingArguments(per_device_train_batch_size=1, gradient_accumulation_steps=4, **default_args)\n",
        "\n",
        "trainer = Trainer(model=model, args=training_args, train_dataset=ds)\n",
        "result = trainer.train()\n",
        "print_summary(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "id": "ozLH_DRw4fJ3",
        "outputId": "15a1d8c8-3986-497f-e16b-1dc8afb5841e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='128' max='128' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [128/128 03:14, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time: 196.44\n",
            "Samples/second: 2.61\n",
            "GPU memory occupied: 7343 MB.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# gradient checkpointing\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=1, gradient_accumulation_steps=4, gradient_checkpointing=True, **default_args\n",
        ")\n",
        "\n",
        "trainer = Trainer(model=model, args=training_args, train_dataset=ds)\n",
        "result = trainer.train()\n",
        "print_summary(result)"
      ],
      "metadata": {
        "id": "uu-T0K6e4fMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fp16 training\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    gradient_checkpointing=True,\n",
        "    fp16=True,\n",
        "    **default_args,\n",
        ")\n",
        "\n",
        "trainer = Trainer(model=model, args=training_args, train_dataset=ds)\n",
        "result = trainer.train()\n",
        "print_summary(result)"
      ],
      "metadata": {
        "id": "VyjZaQwO6hnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fp16 + ga + gc\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    gradient_checkpointing=True,\n",
        "    fp16=True,\n",
        "    **default_args,\n",
        ")\n",
        "\n",
        "trainer = Trainer(model=model, args=training_args, train_dataset=ds)\n",
        "result = trainer.train()\n",
        "print_summary(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "id": "ZgQCVeJi65aU",
        "outputId": "af8ef824-3ae5-4c3e-f4aa-ad2814dc934e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='128' max='128' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [128/128 01:49, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time: 110.33\n",
            "Samples/second: 4.64\n",
            "GPU memory occupied: 6329 MB.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "TrainingArguments(bf16=True)\n",
        "\n",
        "\n",
        "import torch\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "TrainingArguments(tf32=True)\n",
        "```"
      ],
      "metadata": {
        "id": "aKVw51Bc7BC2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# adafactor\n",
        "\n",
        "training_args = TrainingArguments(per_device_train_batch_size=4, optim=\"adafactor\", **default_args)\n",
        "\n",
        "trainer = Trainer(model=model, args=training_args, train_dataset=ds)\n",
        "result = trainer.train()\n",
        "print_summary(result)"
      ],
      "metadata": {
        "id": "btRqNUGk6vS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# adafactor + fp16 + ga + gc\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    gradient_checkpointing=True,\n",
        "    fp16=True,\n",
        "    optim=\"adafactor\",\n",
        "    **default_args,\n",
        ")\n",
        "\n",
        "trainer = Trainer(model=model, args=training_args, train_dataset=ds)\n",
        "result = trainer.train()\n",
        "print_summary(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "L2ahWsqM6vVQ",
        "outputId": "d63275a9-a7f3-4912-a639-ff834b2c0607"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='128' max='128' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [128/128 01:55, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time: 116.75\n",
            "Samples/second: 4.38\n",
            "GPU memory occupied: 3897 MB.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8 bit adam\n",
        "\n",
        "import bitsandbytes as bnb\n",
        "from torch import nn\n",
        "from transformers.trainer_pt_utils import get_parameter_names\n",
        "\n",
        "training_args = TrainingArguments(per_device_train_batch_size=4, **default_args)\n",
        "\n",
        "decay_parameters = get_parameter_names(model, [nn.LayerNorm])\n",
        "decay_parameters = [name for name in decay_parameters if \"bias\" not in name]\n",
        "optimizer_grouped_parameters = [\n",
        "    {\n",
        "        \"params\": [p for n, p in model.named_parameters() if n in decay_parameters],\n",
        "        \"weight_decay\": training_args.weight_decay,\n",
        "    },\n",
        "    {\n",
        "        \"params\": [p for n, p in model.named_parameters() if n not in decay_parameters],\n",
        "        \"weight_decay\": 0.0,\n",
        "    },\n",
        "]\n",
        "\n",
        "optimizer_kwargs = {\n",
        "    \"betas\": (training_args.adam_beta1, training_args.adam_beta2),\n",
        "    \"eps\": training_args.adam_epsilon,\n",
        "}\n",
        "optimizer_kwargs[\"lr\"] = training_args.learning_rate\n",
        "adam_bnb_optim = bnb.optim.Adam8bit(\n",
        "    optimizer_grouped_parameters,\n",
        "    betas=(training_args.adam_beta1, training_args.adam_beta2),\n",
        "    eps=training_args.adam_epsilon,\n",
        "    lr=training_args.learning_rate,\n",
        ")\n",
        "\n",
        "# trainer = Trainer(model=model, args=training_args, train_dataset=ds, optimizers=(adam_bnb_optim, None))\n",
        "# result = trainer.train()\n",
        "# print_summary(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kvBTcd36vXm",
        "outputId": "9e91f300-47dc-4c05-bb53-5d79dbbcf909"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: /usr/lib64-nvidia did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('http'), PosixPath('8013'), PosixPath('//172.28.0.1')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('--logtostderr --listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https'), PosixPath('//colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-t4-vhm-3bvuz2y6z3vge --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/env/python')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//ipykernel.pylab.backend_inline'), PosixPath('module')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8 bit adam + fp16 + ga + gc\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    gradient_checkpointing=True,\n",
        "    fp16=True,\n",
        "    **default_args,\n",
        ")\n",
        "\n",
        "trainer = Trainer(model=model, args=training_args, train_dataset=ds, optimizers=(adam_bnb_optim, None))\n",
        "result = trainer.train()\n",
        "print_summary(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "5g7SRkCG7sCi",
        "outputId": "a38f4f0e-10d2-44bf-d218-9cd85ca5bcf1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='128' max='128' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [128/128 01:39, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time: 100.56\n",
            "Samples/second: 5.09\n",
            "GPU memory occupied: 4417 MB.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x9-Rkf39-QxJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}